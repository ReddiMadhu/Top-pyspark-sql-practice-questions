{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1e5b0f-03ed-4234-baaf-8930aadd306b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[Total Utilization Time - DataLemur](https://datalemur.com/questions/total-utilization-time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52708bb-0753-4ddd-84d1-95786791833c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"server_id\", IntegerType(), True),\n",
    "    StructField(\"session_status\", StringType(), True),\n",
    "    StructField(\"status_time\", StringType(), True)  # initially as string to parse later\n",
    "])\n",
    "\n",
    "# Raw data as list of tuples\n",
    "data = [\n",
    "    (1, \"start\", \"08/02/2022 10:00:00\"),\n",
    "    (1, \"stop\", \"08/04/2022 10:00:00\"),\n",
    "    (1, \"stop\", \"08/13/2022 19:00:00\"),\n",
    "    (1, \"start\", \"08/13/2022 10:00:00\"),\n",
    "    (3, \"stop\", \"08/19/2022 10:00:00\"),\n",
    "    (3, \"start\", \"08/18/2022 10:00:00\"),\n",
    "    (5, \"stop\", \"08/19/2022 10:00:00\"),\n",
    "    (4, \"stop\", \"08/19/2022 14:00:00\"),\n",
    "    (4, \"start\", \"08/16/2022 10:00:00\"),\n",
    "    (3, \"stop\", \"08/14/2022 10:00:00\"),\n",
    "    (3, \"start\", \"08/06/2022 10:00:00\"),\n",
    "    (2, \"stop\", \"08/24/2022 10:00:00\"),\n",
    "    (2, \"start\", \"08/17/2022 10:00:00\"),\n",
    "    (5, \"start\", \"08/14/2022 21:00:00\")\n",
    "]\n",
    "\n",
    "# Create DataFrame with string timestamps\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert status_time string to timestamp type with the given format MM/dd/yyyy HH:mm:ss\n",
    "df = df.withColumn(\"status_time\", to_timestamp(\"status_time\", \"MM/dd/yyyy HH:mm:ss\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20edff6f-8297-4c6d-a1e2-8bf627ef433b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=df.sort(\"server_id\",\"status_time\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f8010e-21e4-4a85-a481-31f1350f4084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, sum as spark_sum, floor, datediff\n",
    "\n",
    "# Window partitioned by server_id, ordered by status_time\n",
    "w = Window.partitionBy(\"server_id\").orderBy(\"status_time\")\n",
    "\n",
    "# Identify start/stop pairs and calculate uptime in days\n",
    "df2 = df1.withColumn(\"prev_status\", lag(\"session_status\").over(w)) \\\n",
    "         .withColumn(\"prev_time\", lag(\"status_time\").over(w)) \\\n",
    "         .where((col(\"session_status\") == \"stop\") & (col(\"prev_status\") == \"start\")) \\\n",
    "         .withColumn(\"uptime_days\", datediff(col(\"status_time\"), col(\"prev_time\")))\n",
    "\n",
    "# Sum all uptime days across all servers\n",
    "result = df2.agg(spark_sum(\"uptime_days\").alias(\"total_uptime_days\"))\n",
    "\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Server Utilization Time",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
